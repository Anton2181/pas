# Tango task distributor

This repo houses the offline pipeline that schedules weekly tango school tasks. The data snapshots in the root (CSV + backend metadata) are kept up to date so you can run the encoder + solver locally without hitting Google Sheets.

## Workflow overview

1. `python3 extractor.py`
   - Pulls the latest Google Sheet export when needed and refreshes `components_all.csv` / `backend.csv`.
   - The checked-in CSVs already contain the dataset that triggered the current round of debugging, so you can skip this step unless you have new data.
2. `python3 encode_sat_from_components.py`
   - Builds `schedule.opb`, `varmap.json`, and `stats.txt`.
   - Automatically widens candidate pools via leader/follower sibling links, merges "Both" role expansions, and reports which families are scarcity-limited (see `auto_soften_families` in `varmap.json`).
3. `python3 run_solver.py --opb schedule.opb --log logs/solver.log`
   - Runs SAT4J with a **hard 120s timeout** so the workflow never hangs. When the limit triggers the wrapper now sends `SIGINT` (same as pressing <kbd>Ctrl+C</kbd>) so SAT4J can print its best-so-far assignment before the process is force-killed. Adjust `--timeout` if you want a different wall clock limit. The wrapper mirrors stdout to the terminal and to the specified log file, returning exit code `124` when it ultimately kills the solver.
   - Every `v ...` model that SAT4J prints is captured automatically. The wrapper writes them to `models.txt` and immediately invokes `consume_saved_models.py` so `assigned_optimal.csv`, `models_summary.csv`, `loads_by_person.csv`, `fairness_plots_*.png`, and `penalties_activated.csv` are refreshed without manual copy/paste. Pass `--skip-consume` if you only want the solver log. (If the solver times out before it emits a `v ...` line, the wrapper now delivers `SIGINT`, waits `--interrupt-grace` seconds for the best model to arrive, and only then reports that no evaluation was run.)

## Script inputs and generated artifacts

The CSV/JSON artifacts under version control are **always** regenerated by these scripts—please do not hand-edit them. Re-run the relevant command instead so downstream files stay consistent.

| Script | Reads | Writes | Notes |
| --- | --- | --- | --- |
| `extractor.py` | Google Sheet (via `task_assignment.csv` / `backend.csv` caches) | `components_all.csv`, `backend.csv`, `task_assignment.csv`, `decision_log.csv`, `remaining_unassigned_only.csv` | Only needed when the source spreadsheet changes. Produces the canonical component/candidate snapshot consumed by later steps. |
| `encode_sat_from_components.py` | `components_all.csv`, `backend.csv` | `schedule.opb`, `varmap.json`, `stats.txt` | Never edit `schedule.opb`/`varmap.json` manually—rerun the encoder whenever inputs or weights change. |
| `run_solver.py` | `schedule.opb` (plus `varmap.json` when consuming) | `logs/solver.log`, `models.txt`, `assigned_optimal.csv`, `models_summary.csv`, `loads_by_person.csv`, `penalties_activated.csv`, `cooldown_debug_by_pf.csv`, `fairness_plots_*.png` | Wraps SAT4J, enforces timeouts, and chains into `consume_saved_models.py`. Passing `--skip-consume` limits output to the solver log and raw models. |
| `consume_saved_models.py` *(called automatically by `run_solver.py`)* | `models.txt`, `varmap.json`, `components_all.csv` | Same analytics artifacts listed above | Safe to run manually if you want to re-summarize an existing `models.txt` without invoking the solver again. |

## Encoder details

- Soft constraints such as cooldown ladders, repeat caps, and fairness tiers are documented in `docs/encoder.md`.
- `stats.txt` now records whether auto-softening skipped any penalties for candidate-starved families. Combine that list with `penalties_activated.csv` to see whether the remaining penalties concentrate in better-staffed families.
- `varmap.json` contains every objective selector plus a copy of `CONFIG`, so it is the single source of truth for the weights used in each run.

## When something looks off

1. **Penalties feel unreasonably high even when weights change.** Check `varmap.json.auto_soften_families`. Families that do *not* appear there still pay full cooldown/repeat costs; if they only have one or two eligible people, widen their candidate pool in `components_all.csv` or the backend roles so they qualify for auto-softening.
2. **Solver output takes too long.** Re-run via `python3 run_solver.py --timeout 90` (or any limit you prefer). The wrapper will stop SAT4J automatically, send `SIGINT` so the solver can flush the best-known `v ...` line, and keep the downstream CSVs in sync.
3. **Need more context?** `docs/encoder.md` describes every artifact the encoder produces and how to inspect it.
