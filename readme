# Tango task distributor

This repo houses the offline pipeline that schedules weekly tango school tasks. The data snapshots in the root (CSV + backend metadata) are kept up to date so you can run the encoder + solver locally without hitting Google Sheets. All solver/analysis outputs (`schedule.opb`, `varmap.json`, `stats.txt`, assignment CSVs, fairness plots, etc.) are generated on demand and now live in `.gitignore`, so expect them to appear as untracked files whenever you run the pipeline.

## Workflow overview

First install the Python dependencies (including matplotlib/networkx for the graphs) with:

```bash
python3 -m pip install -r requirements.txt
```

If you plan to run the automated rule tests, add the dev extras:

```bash
python3 -m pip install -r requirements-dev.txt
```

1. `python3 extractor.py`
   - Pulls the latest Google Sheet export when needed and refreshes `components_all.csv` / `backend.csv`.
   - The checked-in CSVs already contain the dataset that triggered the current round of debugging, so you can skip this step unless you have new data.
2. `python3 encode_sat_from_components.py`
   - Builds `schedule.opb`, `varmap.json`, and `stats.txt` (all gitignored artifacts that are safe to regenerate at will) and keeps `family_registry.json` in sync. The registry records every canonical sibling/family token exactly once, along with the component IDs and task names that contributed to that family, so you can disambiguate identically named families across runs and add a human-friendly alias that will show up in future stats/diagnostics.
   - Automatically widens candidate pools via leader/follower sibling links, merges "Both" role expansions, reports which families are scarcity-limited (see `auto_soften_families` in `varmap.json`), and now reuses any aliases you entered in the registry whenever a family is mentioned in the outputs.
3. `python3 run_solver.py --opb schedule.opb --log logs/solver.log`
   - Runs SAT4J with a **hard 120s timeout** so the workflow never hangs. When the limit triggers the wrapper now sends `SIGINT` (same as pressing <kbd>Ctrl+C</kbd>) so SAT4J can print its best-so-far assignment before the process is force-killed. Adjust `--timeout` if you want a different wall clock limit. The wrapper mirrors stdout to the terminal and to the specified log file, returning exit code `124` when it ultimately kills the solver.
   - Every `v ...` model that SAT4J prints is captured automatically. The wrapper writes them to `models.txt` and immediately invokes `consume_saved_models.py` so `assigned_optimal.csv`, `models_summary.csv`, `loads_by_person.csv`, `fairness_plots_*.png`, and `penalties_activated.csv` are refreshed without manual copy/paste. These outputs are also gitignored, so they can be overwritten freely between experiments. Pass `--skip-consume` if you only want the solver log. (If the solver times out before it emits a `v ...` line, the wrapper now delivers `SIGINT`, waits `--interrupt-grace` seconds for the best model to arrive, and only then reports that no evaluation was run.)
4. `python3 summarize_results.py`
   - Reads the freshly-generated CSVs and prints the best-known objective, assignment counts, penalty tallies, and top/bottom load holders. Use `--top N` to change how many entries show up in each section or override the file paths if you are comparing alternate runs.
5. `python3 visualize_components.py`
   - Optional visualization step that uses `networkx` + `matplotlib` to draw multiple variants of the component exclusivity graph. Nodes are heat-mapped by candidate availability (with day-specific border colors) so scarce slots pop immediately, labels can be toggled via `--label-mode`, and a calendar layout joins the default `grid`/`spring`/`component` projections to reduce clutter. Graph PNGs now land inside `components_graphs/` (override with `--out-dir`) to keep them grouped, and the `--out-prefix` flag only controls the filename stem. Every run also emits a trio of analysis charts inside `components_analysis/` that summarize candidate distributions, day/week scarcity, and the relationship between candidate depth and conflict degree. Use `--analysis-dir` or `--skip-analysis` to redirect/omit those PNGs.
6. `python3 report_assignments.py`
   - Consumes `assigned_optimal.csv` plus the base `components_all.csv` snapshot and writes `reports/assignment_report.csv` and `reports/assignment_report.txt`. The report lists per-person totals, priority versus non-priority splits, repeat-heavy families, and whether each person had access to a priority slot but failed to receive one. Re-run it every time you regenerate assignments so you can track whether fairness tweaks are spreading the load.
7. `python3 run_weight_experiments.py --plans plans.json`
   - Optional parallel harness that spawns multiple encode + solve cycles with different weight overrides. Each plan writes its own artifacts under `experiments/<plan>/` and the script prints a CSV summary comparing objectives and penalty counts, making it easy to see which weight mix reduces penalties.

## Script inputs and generated artifacts

The CSV/JSON artifacts under version control are **always** regenerated by these scripts—please do not hand-edit them. Re-run the relevant command instead so downstream files stay consistent. Solver outputs (the ones listed below as "Writes" for `encode_sat_from_components.py`, `run_solver.py`, `summarize_results.py`, and `visualize_components.py`) are generated locally and excluded from version control to keep PRs text-only.

| Script | Reads | Writes | Notes |
| --- | --- | --- | --- |
| `extractor.py` | Google Sheet (via `task_assignment.csv` / `backend.csv` caches) | `components_all.csv`, `backend.csv`, `task_assignment.csv`, `decision_log.csv`, `remaining_unassigned_only.csv` *(gitignored outputs aside from the canonical CSVs)* | Only needed when the source spreadsheet changes. Produces the canonical component/candidate snapshot consumed by later steps. |
| `encode_sat_from_components.py` | `components_all.csv`, `backend.csv`, `family_registry.json` | `schedule.opb`, `varmap.json`, `stats.txt` *(gitignored)* plus a rewritten `family_registry.json` for any newly seen families, merging their component IDs and task names | Never edit `schedule.opb`/`varmap.json` manually—rerun the encoder whenever inputs or weights change. Update `family_registry.json` only to fill in aliases; the encoder handles adding new canonical keys and component/task lists automatically. |
| `run_solver.py` | `schedule.opb` (plus `varmap.json` when consuming) | `logs/solver.log`, `models.txt`, `assigned_optimal.csv`, `models_summary.csv`, `loads_by_person.csv`, `penalties_activated.csv`, `cooldown_debug_by_pf.csv`, `fairness_plots_*.png` *(all gitignored)* | Wraps SAT4J, enforces timeouts, and chains into `consume_saved_models.py`. Passing `--skip-consume` limits output to the solver log and raw models. |
| `summarize_results.py` | `models_summary.csv`, `penalties_activated.csv`, `loads_by_person.csv`, `assigned_optimal.csv` | *(stdout report)* | Convenience helper that answers “what’s our current best?” without opening spreadsheets. |
| `visualize_components.py` | `components_all.csv`, `backend.csv` | `components_graphs/components_graph_<layout>.png` (one per requested layout) and `components_analysis/components_graph_*.png` (histogram/heatmap/scatter) *(all gitignored)* | Draws multi-layout exclusivity graphs with candidate-based heatmaps, stores them in their own directory, and exports additional availability charts so you can inspect conflicts and scarcity patterns from several angles. |
| `report_assignments.py` | `assigned_optimal.csv`, `components_all.csv` | `reports/assignment_report.csv`, `reports/assignment_report.txt` *(gitignored)* | Post-solve helper that summarizes per-person totals, repeat-heavy families, priority coverage, and eligibility so you can spot uneven load or missed priority opportunities without opening spreadsheets. |
| `run_weight_experiments.py` | `components_all.csv`, `backend.csv`, `family_registry.json`, optional `plans.json`, optional base override JSON | `experiments/<plan>/schedule.opb`, `experiments/<plan>/models_summary.csv`, `experiments/<plan>/penalties.csv` *(gitignored)* plus a stdout CSV comparing objectives/penalties across plans | Launches multiple encoder+solver runs in parallel with per-plan weight overrides so you can quickly gauge how weight changes affect penalty counts. |
| `consume_saved_models.py` *(called automatically by `run_solver.py`)* | `models.txt`, `varmap.json`, `components_all.csv` | Same analytics artifacts listed above | Safe to run manually if you want to re-summarize an existing `models.txt` without invoking the solver again. |

## Encoder details

- Soft constraints such as cooldown ladders, repeat caps, and fairness tiers are documented in `docs/encoder.md`.
- Detailed descriptions of every hard/soft rule now live in `docs/rules.md`, including the updated explanation of the top-priority miss guard. It’s still a soft constraint, but the new `W_PRIORITY_MISS` selectors heavily penalize leaving an eligible dancer without a top-priority slot.
- `stats.txt` now records whether auto-softening skipped any penalties for candidate-starved families. Combine that list with `penalties_activated.csv` to see whether the remaining penalties concentrate in better-staffed families. The same section also echoes which alias (from `family_registry.json`) was used for each scarce family so you can match the human name to the canonical key shown elsewhere.
- `family_registry.json` is the permanent home for those aliases. Every time the encoder encounters a new canonical family token it rewrites/extends the registry with that token, the component IDs using it, and the task names tied to those components, leaving the alias blank so you can rename it later. Once you fill in the alias field, all encoder outputs will use your label (while still tracking the canonical token for debugging).
- Fairness ladders are now availability-aware: `varmap.json.fairness_targets` shows each person’s individualized target, scaled by how many AUTO slots they could realistically cover. People with few opportunities no longer get hammered by Tier-6 under-load selectors, while high-capacity dancers inherit larger targets so the solver naturally spreads work. The defaults keep `FAIR_MEAN_MULTIPLIER = 1.0` and `FAIR_OVER_START_DELTA = 0`, so the global average load becomes the anchor before availability scaling tweaks each person’s target.
- `varmap.json` contains every objective selector plus a copy of `CONFIG`, so it is the single source of truth for the weights used in each run.

## When something looks off

1. **Penalties feel unreasonably high even when weights change.** Check `varmap.json.auto_soften_families` *and* the fairness section in `stats.txt`. Scarce families now auto-soften, and Tier-6 targets scale by candidate availability, so if you still see a flood of fairness penalties it usually means certain dancers have many eligible slots but only a handful of assignments—inspect `reports/assignment_report.csv` to see who is hoarding repeats.
2. **Solver output takes too long.** Re-run via `python3 run_solver.py --timeout 90` (or any limit you prefer). The wrapper will stop SAT4J automatically, send `SIGINT` so the solver can flush the best-known `v ...` line, and keep the downstream CSVs in sync.
3. **Need more context?** `docs/encoder.md` describes every artifact the encoder produces and how to inspect it.
4. **Wondering whether priority opportunities were missed?** `report_assignments.py` writes a CSV/text summary that lists per-person totals, repeat families, and whether someone could have landed a priority slot but didn’t. That’s the quickest way to spot coverage gaps after each solver run.
5. **Need to double-check a rule combination?** Run `pytest` to execute the miniature fixtures that cover two-day modes, auto-softening, repeat penalties, fairness scaling, the report generator, and the end-to-end solver cycle. The fixtures live under `tests/` and do not touch the real data.
