# Tango task distributor

This repo houses the offline pipeline that schedules weekly tango school tasks. The data snapshots in the root (CSV + backend metadata) are kept up to date so you can run the encoder + solver locally without hitting Google Sheets. All solver/analysis outputs (`schedule.opb`, `varmap.json`, `stats.txt`, assignment CSVs, fairness plots, etc.) are generated on demand and now live in `.gitignore`, so expect them to appear as untracked files whenever you run the pipeline.

## Workflow overview

First install the Python dependencies (including matplotlib/networkx for the graphs) with:

```bash
python3 -m pip install -r requirements.txt
```

If you plan to run the automated rule tests, add the dev extras:

```bash
python3 -m pip install -r requirements-dev.txt
```

1. `python3 extractor.py`
   - Pulls the latest Google Sheet export when needed and refreshes `components_all.csv` / `backend.csv`.
   - The checked-in CSVs already contain the dataset that triggered the current round of debugging, so you can skip this step unless you have new data.
2. `python3 encode_sat_from_components.py`
   - Builds `schedule.opb`, `varmap.json`, and `stats.txt` (all gitignored artifacts that are safe to regenerate at will).
   - Automatically widens candidate pools via leader/follower sibling links, merges "Both" role expansions, and reports which families are scarcity-limited (see `auto_soften_families` in `varmap.json`).
3. `python3 run_solver.py --opb schedule.opb --log logs/solver.log`
   - Runs SAT4J with a **hard 120s timeout** so the workflow never hangs. When the limit triggers the wrapper now sends `SIGINT` (same as pressing <kbd>Ctrl+C</kbd>) so SAT4J can print its best-so-far assignment before the process is force-killed. Adjust `--timeout` if you want a different wall clock limit. The wrapper mirrors stdout to the terminal and to the specified log file, returning exit code `124` when it ultimately kills the solver.
   - Every `v ...` model that SAT4J prints is captured automatically. The wrapper writes them to `models.txt` and immediately invokes `consume_saved_models.py` so `assigned_optimal.csv`, `models_summary.csv`, `loads_by_person.csv`, `fairness_plots_*.png`, and `penalties_activated.csv` are refreshed without manual copy/paste. These outputs are also gitignored, so they can be overwritten freely between experiments. Pass `--skip-consume` if you only want the solver log. (If the solver times out before it emits a `v ...` line, the wrapper now delivers `SIGINT`, waits `--interrupt-grace` seconds for the best model to arrive, and only then reports that no evaluation was run.)
4. `python3 summarize_results.py`
   - Reads the freshly-generated CSVs and prints the best-known objective, assignment counts, penalty tallies, and top/bottom load holders. Use `--top N` to change how many entries show up in each section or override the file paths if you are comparing alternate runs.
5. `python3 visualize_components.py`
   - Optional visualization step that uses `networkx` + `matplotlib` to draw multiple variants of the component exclusivity graph. Nodes are now heat-mapped by candidate availability (with day-specific border colors) so scarce slots pop immediately, labels can be toggled via `--label-mode`, and a calendar layout joins the default `grid`/`spring`/`component` projections to reduce clutter. Every run also emits a trio of analysis charts (`components_analysis_*.png`) that summarize candidate distributions, day/week scarcity, and the relationship between candidate depth and conflict degree. Use `--analysis-prefix` to redirect those PNGs or `--skip-analysis` if you only care about the graphs.

## Script inputs and generated artifacts

The CSV/JSON artifacts under version control are **always** regenerated by these scripts—please do not hand-edit them. Re-run the relevant command instead so downstream files stay consistent. Solver outputs (the ones listed below as "Writes" for `encode_sat_from_components.py`, `run_solver.py`, `summarize_results.py`, and `visualize_components.py`) are generated locally and excluded from version control to keep PRs text-only.

| Script | Reads | Writes | Notes |
| --- | --- | --- | --- |
| `extractor.py` | Google Sheet (via `task_assignment.csv` / `backend.csv` caches) | `components_all.csv`, `backend.csv`, `task_assignment.csv`, `decision_log.csv`, `remaining_unassigned_only.csv` *(gitignored outputs aside from the canonical CSVs)* | Only needed when the source spreadsheet changes. Produces the canonical component/candidate snapshot consumed by later steps. |
| `encode_sat_from_components.py` | `components_all.csv`, `backend.csv` | `schedule.opb`, `varmap.json`, `stats.txt` *(gitignored)* | Never edit `schedule.opb`/`varmap.json` manually—rerun the encoder whenever inputs or weights change. |
| `run_solver.py` | `schedule.opb` (plus `varmap.json` when consuming) | `logs/solver.log`, `models.txt`, `assigned_optimal.csv`, `models_summary.csv`, `loads_by_person.csv`, `penalties_activated.csv`, `cooldown_debug_by_pf.csv`, `fairness_plots_*.png` *(all gitignored)* | Wraps SAT4J, enforces timeouts, and chains into `consume_saved_models.py`. Passing `--skip-consume` limits output to the solver log and raw models. |
| `summarize_results.py` | `models_summary.csv`, `penalties_activated.csv`, `loads_by_person.csv`, `assigned_optimal.csv` | *(stdout report)* | Convenience helper that answers “what’s our current best?” without opening spreadsheets. |
| `visualize_components.py` | `components_all.csv`, `backend.csv` | `components_graph_<layout>.png` files (one per requested layout) and `components_analysis_*.png` (histogram/heatmap/scatter) *(all gitignored)* | Draws multi-layout exclusivity graphs with candidate-based heatmaps and exports additional availability charts so you can inspect conflicts and scarcity patterns from several angles. |
| `consume_saved_models.py` *(called automatically by `run_solver.py`)* | `models.txt`, `varmap.json`, `components_all.csv` | Same analytics artifacts listed above | Safe to run manually if you want to re-summarize an existing `models.txt` without invoking the solver again. |

## Encoder details

- Soft constraints such as cooldown ladders, repeat caps, and fairness tiers are documented in `docs/encoder.md`.
- `stats.txt` now records whether auto-softening skipped any penalties for candidate-starved families. Combine that list with `penalties_activated.csv` to see whether the remaining penalties concentrate in better-staffed families.
- `varmap.json` contains every objective selector plus a copy of `CONFIG`, so it is the single source of truth for the weights used in each run.

## When something looks off

1. **Penalties feel unreasonably high even when weights change.** Check `varmap.json.auto_soften_families`. Families that do *not* appear there still pay full cooldown/repeat costs; if they only have one or two eligible people, widen their candidate pool in `components_all.csv` or the backend roles so they qualify for auto-softening.
2. **Solver output takes too long.** Re-run via `python3 run_solver.py --timeout 90` (or any limit you prefer). The wrapper will stop SAT4J automatically, send `SIGINT` so the solver can flush the best-known `v ...` line, and keep the downstream CSVs in sync.
3. **Need more context?** `docs/encoder.md` describes every artifact the encoder produces and how to inspect it.
4. **Need to double-check a rule combination?** Run `pytest` to execute the miniature fixtures that cover two-day modes, auto-softening, repeat penalties, and the end-to-end solver cycle. The fixtures live under `tests/` and do not touch the real data.
